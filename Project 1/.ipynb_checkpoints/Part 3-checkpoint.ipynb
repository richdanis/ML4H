{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3071a0",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0312896",
   "metadata": {},
   "source": [
    "### Q1: How consistent were the different interpretable/explainable methods? Did they find similar patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b93d3",
   "metadata": {},
   "source": [
    "#### Part 1 \n",
    "We notice, the feature importances of logistic regression, the decision tree and the 2 layer NN show some differences.\n",
    "\n",
    "- For example the decision tree has only a few important features, with more than half of them being very small.\n",
    "- On the contrary in the case of logistic regression, we notice a more even spread of feature importances.\n",
    "- For the neural additive models: we get a totally different picture with only 4 features having non-zero importance.\n",
    "\n",
    "Overall we observed that, there is some overlap of features of high importances, but they do not completely align across all methods used.\n",
    "\n",
    "#### Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba8ddc",
   "metadata": {},
   "source": [
    "### Q2: Given the “interpretable” or “explainable” results of one of the models, how would you explain and present them to a broad audience? Pick one example per part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410749b3",
   "metadata": {},
   "source": [
    "Part 1: Given some numerical tabular data, logistic regression derives coefficients for each feature of the data sample. The magnitudes of these coefficents are explainable by themself since they are only multiplied with the features and then summed up to get the output. \n",
    "Thus if the input features are of comparable size (normalization), then the coefficients can explain how important individual features are to arrive at a decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984789b",
   "metadata": {},
   "source": [
    "Part 2: GradCAM is a method used for models with images as inputs. \n",
    "It highlights regions of interest in the images, which the model might use for its decision.\n",
    "For example if a model is used to discriminate whether an image shows a dog or a cat, in the case of an image of a dog GradCAM is likely highlight the face and ears of the dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72a3d7",
   "metadata": {},
   "source": [
    "### Q3: Did you encounter a tradeoff between accuracy and interpretability/explainability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d70cdb",
   "metadata": {},
   "source": [
    "In Part 1, for the Neural Additive Models (NAMs) there was such a tradeoff.\n",
    "A simple 2 layer neural network performed better than the NAM in F1 and Balanced Accuracy score.\n",
    "But on the other hand the NAM more interpretable in terms of feature importances. \n",
    "Because it only had 4 non-zero absolute feature importances across all samples and for the 2 layer neural network the picture was not as clear, i.e. each feature had at least some contribution to the output. <br>\n",
    "Similiary both logistic regression and the decision performed worse than the 2 layer neural network as well.\n",
    "But one could argue that they are more interpretable given their simpler structure and already built-in feature importances with the magnitude of the log coefficients and the gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f2d9b",
   "metadata": {},
   "source": [
    "### Q4: Do your findings from the interpretability/explainability methods align with the current medical knowledge about these diseases? You may take inspiration from the references of the project presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc858d9",
   "metadata": {},
   "source": [
    "####  Most important features for Heart Disease\n",
    "In our case, the different models for heart disease were not that congruent in terms of feature importance. Nevertheless, one could observe a trend of features which were quite common among the different models such as: Up, ASY and SEX. Comparing these to the current medical knowledge, they roughly align with that since for example an up slope in case of ST depression during an electrocardiogram may indicate cardiac ischemia in combination with different cardiac symptoms. Furthermore, sex is a predictive feature as well, due to the fact that males are more oftenly affected by heart disease than women. Additionally, asymptomatic chestpain is a characteristic issue of heart related disease as well. \n",
    "\n",
    "However, in medical literature the above named features do not count to the most indicative risk factors known. For example, cholesterol, resting blood pressure and diabetes are more often related to cardiac disease than the above named features. Especially that cholesterol was not even in the top 3 feature importances across our different models is suprising, since there is a proven causal relationship between heart disease and plaque formation/narrowing of the arteries due to high cholesterol levels. \n",
    "\n",
    "As a result, our implemented models did not have the highest feature importances related to the most indicative features/ risk factors acknowledged by medicine but at least chose features which can be predictive for heart disease as well. \n",
    "\n",
    "\n",
    "#### Most important features for Pneunomia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Medical Background stuff\n",
    "#### Most important features for Pneunomia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1bdef",
   "metadata": {},
   "source": [
    "### Q5: If you had to deploy one of the methods in practice, which one would you choose and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65dfa6",
   "metadata": {},
   "source": [
    "We would choose to deploy GradCAM. \n",
    "It is easily applied to any model with convolutional layers and the computation is very low, i.e. it is comparable to a single backward pass during training.\n",
    "The resulting heatmaps are often very intuitive and can be interpreted nicely.\n",
    "They can be used to reason, why a model might do wrong predictions and can thus also help to properly identify issues such as bad performance and overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
